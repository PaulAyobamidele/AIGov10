# -*- coding: utf-8 -*-
"""new_governance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HMO0VuPWsyXl5e6ocmDj1i8rX00_gAmd
"""

!pip install arxiv
!pip install PyPDF2
!pip install --upgrade openai
!pip install pymupdf
!pip install auto-gptq==0.2.2
!pip install --upgrade llama-cpp-python

import arxiv
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer

developer_query = "AI ethics OR AI governance OR responsible AI"
max_results = 50
user_query = "ethical implications of artificial intelligence governance"

similarity_model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_similarity(user_query, papers):
    """Compute cosine similarity between user query and paper summaries."""
    user_query_embedding = similarity_model.encode(user_query)
    paper_embeddings = similarity_model.encode([paper['summary'] for paper in papers])
    similarities = np.inner(user_query_embedding, paper_embeddings)
    return similarities

search = arxiv.Search(
    query=developer_query,
    max_results=max_results,
    sort_by=arxiv.SortCriterion.Relevance
)

papers = []
for result in search.results():
    paper_data = {
        "title": result.title,
        "authors": [author.name for author in result.authors],
        "summary": result.summary,
        "pdf_url": result.pdf_url,
        "published_date": result.published.isoformat(),
        "journal_ref": result.journal_ref if result.journal_ref else "Not Published",
        "doi": result.doi if result.doi else "No DOI"
    }
    papers.append(paper_data)

journal_papers = [paper for paper in papers if paper["journal_ref"] != "Not Published"]

if not journal_papers:
    print("No journal papers found in the search results.")
else:
    similarities = compute_similarity(user_query, journal_papers)
    sorted_journal_papers = sorted(zip(similarities, journal_papers), key=lambda x: x[0], reverse=True)

    data = [
        [score, paper['title'], ', '.join(paper['authors']), paper['summary'], paper['pdf_url'], paper['published_date'], paper['journal_ref'], paper['doi']]
        for score, paper in sorted_journal_papers
    ]
    df = pd.DataFrame(data, columns=["Similarity Score", "Title", "Authors", "Summary", "PDF URL", "Published Date", "Journal Reference", "DOI"])

    df.to_csv("filtered_journal_papers.csv", index=False)
    print("Filtered journal papers saved to 'filtered_journal_papers.csv'.")

    print("Top journal papers by similarity score:")
    for score, paper in sorted_journal_papers[:10]:
        print(f"Similarity Score: {score:.4f}")
        print(f"Title: {paper['title']}")
        print(f"Authors: {', '.join(paper['authors'])}")
        print(f"Abstract: {paper ['summary']}")
        print(f"Published Date: {paper['published_date']}")
        print(f"Journal Reference: {paper['journal_ref']}")
        print(f"DOI: {paper['doi']}")
        print(f"PDF URL: {paper['pdf_url']}")
        print("\n---\n")

top_papers = sorted_journal_papers[:5]

top_papers_data = [
    {
        "title": paper['title'],
        "authors": ', '.join(paper['authors']),
        "summary": paper['summary'],
        "pdf_url": paper['pdf_url'],
        "published_date": paper['published_date'],
        "journal_ref": paper['journal_ref'],
        "doi": paper['doi']
    }
    for _, paper in top_papers
]

# Print extracted top 5 papers
print("Top 5 Papers Extracted:")
for idx, paper in enumerate(top_papers_data, 1):
    print(f"{idx}. {paper['title']} (Published: {paper['published_date']})")

# import requests
# import fitz

# def download_and_extract_text(pdf_url):
#     """Download a PDF from the given URL and extract its text."""
#     try:
#         response = requests.get(pdf_url)
#         response.raise_for_status()

#         with open("temp_paper.pdf", "wb") as pdf_file:
#             pdf_file.write(response.content)

#         doc = fitz.open("temp_paper.pdf")
#         full_text = ""
#         for page in doc:
#             full_text += page.get_text()
#         doc.close()
#         return full_text
#     except Exception as e:
#         print(f"Error processing PDF at {pdf_url}: {e}")
#         return ""

# for paper in top_papers_data:
#     paper['full_text'] = download_and_extract_text(paper['pdf_url'])

# print("Full paper text extracted for top papers.")

df = pd.DataFrame(top_papers_data)

print("DataFrame created with the following columns:")
print(df.columns)

df.to_csv("top_papers_full_text.csv", index=False)
print("Top papers with full text saved to 'top_papers_full_text.csv'.")

df

import re

def clean_text(text):
    """Clean text by removing unnecessary whitespace and special characters."""
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'[^\w\s.,;:!?\'\"-]', '', text)
    return text

df['title'] = df['title'].apply(clean_text)
df['authors'] = df['authors'].apply(clean_text)
df['summary'] = df['summary'].apply(clean_text)
# df['full_text'] = df['full_text'].apply(clean_text)

print("Text fields cleaned successfully.")

def word_count(text):
    """Counts the number of words in a given text."""
    return len(text.split())

df['summary_word_count'] = df['summary'].apply(word_count)
# df['full_text_word_count'] = df['full_text'].apply(word_count)

print("Feature engineering completed. Word counts added.")

data_for_summarization = [
    {
        "input_text": (
            f"Title: {row['title']}\n"
            f"Authors: {row['authors']}\n"
            f"Published Date: {row['published_date']}\n"
            f"Journal Reference: {row['journal_ref']}\n"
            f"Summary: {row['summary']}\n"
            # f"Full Text: {row['full_text'][:2000]}..."
        ),
        "meta": {
            "pdf_url": row['pdf_url'],
            "doi": row['doi'],
            "summary_word_count": row['summary_word_count'],
            # "full_text_word_count": row['full_text_word_count']
        }
    }
    for _, row in df.iterrows()
]

print("Data prepared for summarization (Preview of first entry):")
print(data_for_summarization[0])

!pip install langchain==0.0.191
!pip install llama-cpp-python==0.1.66
!pip install sentence-transformers
!pip install huggingface_hub
!pip install auto-gptq==0.2.2
!pip install termcolor

from huggingface_hub import hf_hub_download

!pip install llama-cpp-python

from langchain.llms import LlamaCpp

import logging
import click
import torch
import transformers
import os
import re
import shutil
import subprocess
import requests
from pathlib import Path  # Import Path directly
from auto_gptq import AutoGPTQForCausalLM
from huggingface_hub import hf_hub_download
from langchain.embeddings import HuggingFaceInstructEmbeddings  # Import correct embedding class
from langchain.llms import HuggingFacePipeline, LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    GenerationConfig,
    LlamaForCausalLM,
    LlamaTokenizer,
    LongformerTokenizer,
    pipeline,
)
from rouge import Rouge
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm
from termcolor import colored  # Import colored directly

GREEN = '\033[92m'
END_COLOR = '\033[0m'

try:
    imported_modules = [
        ("logging", logging),
        ("click", click),
        ("torch", torch),
        ("transformers", transformers),
        ("os", os),
        ("re", re),
        ("shutil", shutil),
        ("subprocess", subprocess),
        ("requests", requests),
        ("pathlib", Path),
        ("auto_gptq", AutoGPTQForCausalLM),
        ("huggingface_hub", hf_hub_download),
        ("huggingface_instruct_embeddings", HuggingFaceInstructEmbeddings),
        ("langchain_pipeline", HuggingFacePipeline),
        ("llama_cpp", LlamaCpp),
        ("prompt_template", PromptTemplate),
        ("llm_chain", LLMChain),
        ("transformers_auto_tokenizer", AutoTokenizer),
        ("transformers_auto_model", AutoModelForCausalLM),
        ("transformers_generation_config", GenerationConfig),
        ("transformers_llm_model", LlamaForCausalLM),
        ("transformers_llm_tokenizer", LlamaTokenizer),
        ("transformers_longformer_tokenizer", LongformerTokenizer),
        ("transformers_pipeline", pipeline),
        ("rouge", Rouge),
        ("text_splitter", RecursiveCharacterTextSplitter),
        ("tqdm", tqdm),
        ("termcolor_colored", colored),  # Use colored here
    ]

    print("Module(s) Imported:")
    for module_name, module in imported_modules:
        if module:
            print(f" - {module_name}")

            version = getattr(module, "__version__", None)
            if version:
                print(f"   Version: {GREEN}{version}{END_COLOR}")
except ImportError as e:
    print(f"Failed to import a module: {e}")

def load_model(device_type, model_id, model_basename=None):

    logging.info(f"Loading Model: {model_id}, on: {device_type}")
    logging.info("This action can take a few minutes!")

    if model_basename is not None:
        if ".ggml" in model_basename:
            logging.info("Using Llamacpp for GGML quantized models")
            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)
            max_ctx_size = 4096
            kwargs = {
                "model_path": model_path,
                "n_ctx": max_ctx_size,
                "max_tokens": max_ctx_size,
            }
            if device_type.lower() == "mps":
                kwargs["n_gpu_layers"] = 1000
            if device_type.lower() == "cuda":
                kwargs["n_gpu_layers"] = 1000
                kwargs["n_batch"] = max_ctx_size
            return LlamaCpp(**kwargs)

        else:
            logging.info("Using AutoGPTQForCausalLM for quantized models")

            if ".safetensors" in model_basename:
                # Remove the ".safetensors" ending if present
                model_basename = model_basename.replace(".safetensors", "")

            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            logging.info("Tokenizer loaded")

            model = AutoGPTQForCausalLM.from_quantized(
                model_id,
                model_basename=model_basename,
                use_safetensors=True,
                trust_remote_code=True,
                device="cuda:0",
                use_triton=False,
                quantize_config=None,
            )
    elif (
        device_type.lower() == "cuda"
    ):
        logging.info("Using AutoModelForCausalLM for full models")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        logging.info("Tokenizer loaded")

        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            # max_memory={0: "15GB"} # Uncomment this line with you encounter CUDA out of memory errors
        )
        model.tie_weights()
    else:
        logging.info("Using LlamaTokenizer")
        tokenizer = LlamaTokenizer.from_pretrained(model_id)
        model = LlamaForCausalLM.from_pretrained(model_id)

    generation_config = GenerationConfig.from_pretrained(model_id)

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=2048,
        temperature=0,
        top_p=0.95,
        repetition_penalty=1.15,
        generation_config=generation_config,
    )

    local_llm = HuggingFacePipeline(pipeline=pipe)
    logging.info("Local LLM Loaded")

    return local_llm

import torch
import logging

logging.basicConfig(level=logging.INFO)


DEVICE_TYPE = "cuda" if torch.cuda.is_available() else "cpu"
SHOW_SOURCES = True
logging.info(f"Running on: {DEVICE_TYPE}")
logging.info(f"Display Source Documents set to: {SHOW_SOURCES}")

model_id = "TheBloke/Llama-2-7B-Chat-GGML"
model_basename = "llama-2-7b-chat.ggmlv3.q4_0.bin"

LLM = load_model(device_type=DEVICE_TYPE, model_id=model_id, model_basename=model_basename)

def generate_summary(text_chunk):
    template = """
    Write a concise summary of the text, return your responses with 5 lines that cover the key points of the what was discussed in the paper.
    ```{text}```
    SUMMARY:
    """
    prompt = PromptTemplate(template=template, input_variables=["text"])
    llm_chain = LLMChain(prompt=prompt, llm=LLM)

    summary = llm_chain.run(text_chunk)
    return summary

!huggingface-cli login --token $HUGGING_KEY

!pip install llama-cpp-python tqdm

!wget -O TheBloke-Llama-2-7B-Chat-GGML.bin https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin

from llama_cpp import Llama

model_path = "TheBloke-Llama-2-7B-Chat-GGML.bin"

llm = Llama(model_path=model_path)

response = llm("Summarize this text: Artificial intelligence is a branch of computer science...", max_tokens=100)
print(response["choices"][0]["text"])

import os
print(os.path.exists("path_to/TheBloke-Llama-2-7B-Chat-GGML.bin"))

model_path = "/content/TheBloke-Llama-2-7B-Chat-GGML.bin"

import os
print(os.listdir("/content"))  # Adjust the path as needed

import os
print(os.listdir("."))  # Check the current directory

from tqdm import tqdm
import gc
from llama_cpp import Llama
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pandas as pd  # Ensure pandas is imported for DataFrame operations

# Initialize the model with a larger context window if supported
model_path = "/content/TheBloke-Llama-2-7B-Chat-GGML.bin"
llm = Llama(model_path=model_path, n_ctx=512)  # Adjust the context window as per the model's capabilities

# Text splitter configuration
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,  # Ensure chunks fit within token limits
    chunk_overlap=50,
    length_function=len
)

# Function to generate summaries for a text chunk
def generate_summary(text):
    try:
        response = llm(prompt=text, max_tokens=300, stop=["</s>"])
        return response["choices"][0]["text"].strip()
    except Exception as e:
        return f"Error: {e}"

# Summarization pipeline for all abstracts in a DataFrame
def summarize_dataframe(df, source_column, target_column):
    # Add a new column for summaries if it doesn't exist
    if target_column not in df.columns:
        df[target_column] = ""

    for index, row in tqdm(df.iterrows(), total=len(df), desc="Generating Summaries"):
        try:
            full_text = row[source_column]

            # Validate input text
            if not full_text or not isinstance(full_text, str) or not full_text.strip():
                raise ValueError("Invalid or missing text")

            # Split text into manageable chunks
            chunks = text_splitter.split_text(full_text)

            # Generate summary for each chunk
            all_summaries = [generate_summary(chunk) for chunk in chunks if chunk.strip()]

            # Combine chunk summaries into a single summary
            combined_summary = "\n".join(all_summaries)
            df.at[index, target_column] = combined_summary

        except Exception as e:
            print(f"Error processing row {index}: {e}")
            df.at[index, target_column] = f"Error: {e}"

    return df

# Example usage: Load DataFrame and process summaries
# Ensure the DataFrame `df` has a column named 'abstracts' containing the abstracts
# Replace 'abstracts' with the actual column name containing paper abstracts
df = pd.DataFrame({"abstracts": ["Abstract 1 text...", "Abstract 2 text...", "Abstract 3 text..."]})
df = summarize_dataframe(df, source_column="abstracts", target_column="summary")

# Save the results to a CSV file for review
df.to_csv("summarized_abstracts.csv", index=False)